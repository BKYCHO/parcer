import requests
from bs4 import BeautifulSoup as BS
import re
import csv


# Без юзерагента сайт дает ошибку 404

def get_html(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'}
    r = requests.get(url, headers=headers)
    return r.text

# Поиск максимального количества страниц в запросе

def get_total_pages(html):
    soup = BS(html, 'lxml')
    pages = soup.find_all('a', class_='bloko-button HH-Pager-Control')[-1].get('data-page')
    return int(pages)

def get_page_links(html):
    soup = BS(html, 'lxml')
    links = soup.find('div', class_='vacancy-serp').find_all('a', class_='bloko-link HH-LinkModifier')
    all_links = []
    for link in links:
        url = link.get('href')
        all_links.append(url)

    return all_links

#Records all links in a csv file

def write_csv(all_links):
    with open('hh.csv', 'a', newline="") as f:

        writer = csv.writer(f)
        for row in all_links:
            writer.writerow([row])

def main():
    url = 'https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text=Python&page=1'
    base_url = 'https://spb.hh.ru/search/vacancy?L_is_autosearch=false&area=2&clusters=true&enable_snippets=true&text='
    page_part = '&page='
    query_part = 'Python'

    pages = get_total_pages(get_html(url))  # Вторым значением потом поставить pages+1 в 48 строке
    all_links = []
    for page_number in range(0, 3): # Вторым значением потом поставить pages+1
        url_full = base_url + query_part + page_part + str(page_number)
        html = get_html(url_full)
        get_page_links(html)
        all_links.extend(get_page_links(html))

    write_csv(all_links)
    # print(all_links)
    # value = "url"
    # data = dict.fromkeys(all_links, value)







if __name__ == "__main__":
    main()